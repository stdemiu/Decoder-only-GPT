## 1. Цели проекта

- Реализовать минимально-рабочую архитектуру **GPT-подобной языковой модели** (Decoder-only Transformer) в учебных целях.
- Разбить реализацию на модульные компоненты по лекциям:
  1) BPE-токенизатор (Лекция 2),
  2) Token/Positional Embeddings (Лекция 3),
  3) Masked (Causal) Multi-Head Attention (Лекция 4),
  4) Feed Forward Network (FFN) (Лекция 5),
  5) Сборка в **DecoderBlock** и **GPTLanguageModel**, обучение и инференс.
- Настроить воспроизводимую среду: `venv`, `requirements.txt`, запуск тестов `pytest`.
- Подготовить README и подробный отчёт для сдачи.

---

## 2. Структура проекта
```perl
full/
├─ models/
│ ├─ init.py
│ ├─ token_embeddings.py
│ ├─ positional_embeddings.py
│ ├─ head_attention.py
│ ├─ multihead_attention.py
│ ├─ feed_forward.py
│ └─ model.py # GPTLanguageModel + DecoderBlock
│
├─ examples/
│ ├─ init.py
│ └─ train_and_infer.py # обучение и генерация
│
├─ tests/
│ ├─ init.py (опционально)
│ ├─ conftest.py 
│ └─ test_shapes.py 
│
├─ requirements.txt
├─ pyproject.toml (опц., для pip install -e .)
└─ README.md
```

**Ключевые решения по структуре:**  
- Папки `models/` и `examples/` содержат файлы `__init__.py`, чтобы Python видел их как пакеты.  
- При запуске модульных скриптов используем `python -m ...` из корня проекта (`full/`).  
- Для тестов настроен `tests/conftest.py`, добавляющий корень проекта в `sys.path`.



## 3. Теоретические основы и компоненты (подробно)
### 3.1 Токенизация (Лекция 2, BPE)

Нейросеть оперирует числами, а не текстом. Токенизация превращает строку в последовательность индексов (ID) фиксированного словаря.

#### Идея BPE
- Стартуем со словара из символов корпуса (или байтов).
- Итерируемся: считаем частоты соседних пар токенов в тексте → сливаем самую частую пару в новый токен → добавляем токен в словарь.
- Повторяем, пока размер словаря < vocab_size.

#### Обучение (учебный, символ/байт-уровень)

1. vocab ← все уникальные символы в корпусе (отсортируй, чтобы ID стабильно назначились).
2. tokens_seq ← list(text) — последовательность базовых токенов (символов).
3. Пока len(vocab) < vocab_size:
  * посчитай все пары (t_i, t_{i+1}) в tokens_seq → получи частоты и позицию первого появления (для tie-break).
  * возьми пару с максимальной частотой; при равенстве — где раньше впервые встретилась.
  * слей все вхождения пары в новый токен t_new = a+b.
  * добавь t_new в vocab.
4. Заморозь id2token и token2id.

#### Кодирование (encode)
Жадный разбор слева направо:
* На позиции i ищем самый длинный токен из словаря, совпадающий с text[i:].
* Если ничего длинного не подошло — возьми один символ (он есть в словаре).
* Продвинься на длину найденного токена и повтори.

#### Декодирование (decode)

Собрать по id2token строку обратно (конкатенация токенов).

#### Практические моменты

* Byte-level BPE устойчив к любым алфавитам/эмодзи (в проде так чаще).
* Часто добавляют специальные токены: <bos>, <eos>, <pad>, <unk>.
* Ограничения: BPE не понимает морфологию, может «рубить» по странным местам (но это нормально для GPT).
* Стабильность ID: важно фиксировать порядок токенов для повторяемости результатов.
* Скорость: подсчёт пар — O(|text|) за итерацию; итераций — примерно vocab_size - |alphabet|.

### 3.2 Эмбединги (Лекция 3)
#### TokenEmbeddings

* Таблица размеров (vocab_size, emb_size).
* На вход: тензор индексов x формы (B, S) → на выходе (B, S, E).
* Инициализация обычно ~N(0, 0.02) или Xavier/Uniform.

#### PositionalEmbeddings (learned)

* Таблица размеров (max_seq_len, emb_size).
* На вход: целое seq_len → на выходе матрица (S, E) для позиций [0..S-1].
* Складываем с токен-эмбами: input_emb = tok_emb(x) + pos_emb(seq_len) → форма (B, S, E).
* В оригинальном Transformer ввод часто масштабируют: tok_emb * sqrt(E) — это помогает стабилизировать значения. Не обязательно, но полезно.

#### Альтернатива: синусоидальные позиционные эмбединги

* Не учатся, а вычисляются по формулам sin/cos с разными частотами.
* Плюс — не ограничены max_seq_len «жёстко», но на практике всё равно задают «разумный» максимум.

#### Типичные ловушки

* Несовпадение устройств/типов: убедись, что pos_emb на том же device/dtype, что и tok_emb(x).
* Смещение позиций: позиции начинаются с 0. Если есть <bos>, учитывай это в позициях.
* Переполнение max_seq_len: при генерации обрезай контекст до max_seq_len.

### 3.3 Маскированное мультиголовное внимание (Лекция 4)
#### Один «Head» (каузальное/маскированное)
Пусть x ∈ ℝ^{B×S×E} — вход эмбедингов.

1. Линейные проекции:
  * Q = x Wq, K = x Wk, V = x Wv, где Wq, Wk, Wv ∈ ℝ^{E×H} и H = head_size.
  * Формы: Q,K,V ∈ ℝ^{B×S×H}.

2. Счёт внимания:
  * scores = (Q @ K^T) / sqrt(H), где каждая строка — распределение внимания по прошлым позициям.
  * Форма: scores ∈ ℝ^{B×S×S}.

3. Каузальная маска (треугольная):

  * mask[i,j] = 0, если j > i, иначе 1.
  * Применяем: scores = scores.masked_fill(mask == 0, -inf).

4. Нормализация:

  * A = softmax(scores, dim=-1) → A ∈ ℝ^{B×S×S}.

5. Узвешивание значений:

  * out = A @ V → out ∈ ℝ^{B×S×H}.

Стабильность softmax: часто вычитают максимум по строке перед softmax для численной устойчивости: scores = scores - scores.max(dim=-1, keepdim=True).values.

#### Multi-Head Attention (MHA)

* Несколько голов num_heads = h. Для простоты можно сделать h независимых Head и потом cat по последней оси.
* Конкатенация: concat ∈ ℝ^{B×S×(h·H)} = ℝ^{B×S×E} (обычно выбирают H=E/h, тогда h·H = E).
* Выходная проекция: out = concat @ W_o, W_o ∈ ℝ^{E×E}.
*  Dropout после внимания (опционально): out = dropout(out).

#### Стоимость
* Память и время ~ O(S^2) (из-за матрицы S×S). Вот почему большой контекст дорогой.

#### Типичные ловушки

* Маска и её broadcast: приведи маску к форме, совместимой с scores ((1, S, S) или (B, 1, S, S)), чтобы корректно замаскировать будущие токены.
* Скалирование: деление на sqrt(H) обязательно, иначе softmax «перенасыщается».
* bias=False: в учебных проекциях внимания обычно bias=False — ок.

### 3.4 Feed Forward Network (FFN, Лекция 5)
#### Архитектура

* На каждую позицию по отдельности (позиционно-независимо) применяется одинаковая MLP:
* Linear(E, 4E) → ReLU → Linear(4E, E) → Dropout.
* Выход имеет ту же форму (B, S, E).

#### Замены и улучшения

* В прод-моделях часто GELU вместо ReLU (лучше тренируется).
* Современные GPT используют Gated варианты (SwiGLU / GEGLU): прирост качества.
* Коэффициент расширения «4» — эмпирическое; можно 2–8 в зависимости от бюджета.

#### Ловушки

* Перемешивание осей: линейные слои применяются к последней оси (E), последовательность (S) — это размер батча для MLP.
* Dropout: выключается на инференсе; не забывай model.eval().

### 3.5 Резидуальные связи и LayerNorm

Residual помогает пропускать градиент и сохранять «информацию идентичности».
LayerNorm стабилизирует распределения активаций, что улучшает сходимость.

#### Pre-LN vs Post-LN

##### Pre-LN (часто используем сейчас):
```ini
y = x + Dropout( SubLayer( LN(x) ) )
```

Слой нормализации до подслоя (MHA или FFN). Лучше градиенты на глубине.

##### Post-LN (оригинальный Transformer):
```ini
y = LN( x + Dropout( SubLayer(x) ) )
```

Имеет известные проблемы со стабильностью на глубине без спец-трюков.

##### Где ставим Dropout
После внимания (на out_proj), после FFN, иногда — на суммарные эмбединги.

##### Ловушки

* Неправильный порядок: перемена местами LN и Residual ломает тренинг.
* epsilon: по умолчанию 1e-5 ок; иногда ставят 1e-6.

### 3.6 DecoderBlock и GPTLanguageModel
##### DecoderBlock (Pre-LN вариант)

Пусть вход x ∈ ℝ^{B×S×E}:
1. x ← x + Dropout( MHA( LN(x) ) ) — внимание с маской «видим только прошлое».
2. x ← x + Dropout( FFN( LN(x) ) ) — локальная MLP на каждой позиции.

Выход — та же форма (B, S, E).

##### GPTLanguageModel

1. Ввод: индексы токенов idx ∈ ℝ^{B×S}.
2. Эмбединги: x = tok_emb(idx) + pos_emb(S), x ∈ ℝ^{B×S×E}.
3. Стек блоков: прогоняем через N DecoderBlock.
4. Финальная нормализация: x = LN(x).
5. LM Head: logits = x @ W^T + b, где W ∈ ℝ^{V×E} — матрица эмбедингов словаря (можно тайтить с токен-эмбедами: W = tok_emb.weight).
6. Выход: logits ∈ ℝ^{B×S×V}, где V = vocab_size.

##### Обучение (next-token prediction)

* Лосс — CrossEntropy между логитами позиции t и истинным токеном t+1.
* teacher forcing: подаём правильный «контекст», предсказываем следующий символ.

##### Генерация

* Берём последние max_seq_len токенов (обрезаем контекст).
* Получаем логиты последней позиции (B, V).
* Выбираем следующий ID:
  * greedy: argmax,
  * сэмплинг: категориальное распределение по логитам,
  * добавки: temperature, top-k, top-p (nucleus) — для разнообразия.

##### Ловушки

* Контекст окна: обязательно обрезать до max_seq_len, иначе матрица внимания разрастётся, и pos_emb выйдет за пределы.
* weight tying: экономит параметры и улучшает устойчивость (но не обязателен).
* mask dtype: следи, что маска совместима с dtype логитов (часто float32), а -inf корректно обрабатывается.

| Объект                 | Форма                     |
| ---------------------- | ------------------------- |
| Вход индексов          | `(B, S)`                  |
| Эмбединги токенов      | `(B, S, E)`               |
| Эмбединги позиций      | `(S, E)`                  |
| Q, K, V (одной головы) | `(B, S, H)`               |
| Скоpы `QK^T`           | `(B, S, S)`               |
| Внимание `A`           | `(B, S, S)`               |
| Выход головы `AV`      | `(B, S, H)`               |
| Конкатенация голов     | `(B, S, h·H) = (B, S, E)` |
| Проекция MHA           | `(B, S, E)`               |
| FFN выход              | `(B, S, E)`               |
| LM logits              | `(B, S, V)`               |

